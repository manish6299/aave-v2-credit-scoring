# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WFGKwNj6XV1Uon9C4t-a6fNaoh_dzD0k
"""

!pip install pandas matplotlib seaborn scikit-learn xgboost

import json
import pandas as pd

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load JSON
with open('/content/user-wallet-transactions.json') as f:
    data = json.load(f)

data

"""## Load the datasets"""

# Assuming `data` is your loaded JSON object
df = pd.DataFrame(data)

df

"""## Data preprocessing"""

# Flatten nested fields in actionData
df['amount'] = df['actionData'].apply(lambda x: float(x['amount']))
df['assetSymbol'] = df['actionData'].apply(lambda x: x['assetSymbol'])
df['assetPriceUSD'] = df['actionData'].apply(lambda x: float(x['assetPriceUSD']))
df['usd_value'] = df['amount'] * df['assetPriceUSD']

# Convert timestamp fields
df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
df['createdAt'] = pd.to_datetime(df['createdAt'].apply(lambda x: x['$date']))
df['updatedAt'] = pd.to_datetime(df['updatedAt'].apply(lambda x: x['$date']))

# Optionally flatten _id (Mongo-style)
df['_id'] = df['_id'].apply(lambda x: x['$oid'])

# Drop actionData column if flattened
df.drop(columns=['actionData'], inplace=True)

df

df.info()

df.isnull().sum()

"""### EDA"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='action', data=df)
plt.title("Transaction Action Distribution")
plt.xticks(rotation=45)

plt.show()

sns.histplot(df['usd_value'], bins=50, log_scale=True)
plt.title("USD Value per Transaction")
plt.show()

df.set_index('timestamp').resample('D')['usd_value'].sum().plot(title="Daily Transaction Volume (USD)")
plt.show()

"""## Feature engineering


Engineer features from the transaction data to capture wallet behavior.

**Reasoning**:
I will group the dataframe by userWallet and calculate the total number of transactions, total USD value, average and standard deviation of USD value, first and last transaction timestamps, and number of active days.
"""

wallet_features = df.groupby('userWallet').agg(
    total_txn=('txHash', 'count'),
    total_usd=('usd_value', 'sum'),
    mean_txn_usd=('usd_value', 'mean'),
    std_txn_usd=('usd_value', 'std'),
    first_tx=('timestamp', 'min'),
    last_tx=('timestamp', 'max')
)

wallet_features['active_days'] = (wallet_features['last_tx'] - wallet_features['first_tx']).dt.days + 1
wallet_features['tx_per_day'] = wallet_features['total_txn'] / wallet_features['active_days']
wallet_features.loc[wallet_features['active_days'] == 1, 'tx_per_day'] = wallet_features['total_txn']

"""**Reasoning**:
I will pivot the original DataFrame to count the occurrences of each transaction action for each wallet and then merge this with the `wallet_features` DataFrame, filling missing values with 0.


"""

action_counts = df.pivot_table(index='userWallet', columns='action', values='txHash', aggfunc='count', fill_value=0)
wallet_features = wallet_features.merge(action_counts, left_index=True, right_index=True, how='left').fillna(0)

"""**Reasoning**:
I will calculate the ratio of each action type to the total number of transactions and then calculate the credit score based on the total USD value using a logarithmic transformation and scaling.


"""

# Calculate action ratios
for action in ['deposit', 'borrow', 'liquidationcall', 'redeemunderlying', 'repay']:
    if action in wallet_features.columns:
        wallet_features[f'{action}_ratio'] = wallet_features[action] / wallet_features['total_txn']
    else:
        wallet_features[f'{action}_ratio'] = 0

# Calculate credit score based on total_usd with log scaling
wallet_features['log_total_usd'] = np.log10(wallet_features['total_usd'] + 1)
min_log_usd = wallet_features['log_total_usd'].min()
max_log_usd = wallet_features['log_total_usd'].max()
wallet_features['credit_score'] = ((wallet_features['log_total_usd'] - min_log_usd) / (max_log_usd - min_log_usd)) * 1000

"""## Prepare data for modeling
Select features, split the data into training and testing sets, and scale the features.

**Reasoning**:
Select the features and the target variable, then split the data into training and testing sets, and finally scale the features for model training.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Select features (X) and target (y)
features = ['log_total_usd', 'tx_per_day', 'total_txn'] + [col for col in wallet_features.columns if col.endswith('_ratio')]
X = wallet_features[features]
y = wallet_features['credit_score']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Train and evaluate multiple models

**Reasoning**:
Import necessary libraries and train and evaluate the models.
"""

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Define models
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'XGBoost': xgb.XGBRegressor(random_state=42),
    'Support Vector Regressor': SVR()
}

# Initialize results dictionary
results = {}

# Iterate through models, train, evaluate, and store results
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[name] = (mse, r2)
    print(f"{name} - MSE: {mse:.4f}, R2: {r2:.4f}")

"""## Compare model performance

**Reasoning**:
Create a DataFrame from the results dictionary, sort it by R2 score, and then print the DataFrame and identify the best performing model based on the results.
"""

results_df = pd.DataFrame.from_dict(results, orient='index', columns=['MSE', 'R2'])
results_df = results_df.sort_values(by='R2', ascending=False)
display(results_df)

best_model_name = results_df.index[0]
best_r2 = results_df['R2'].iloc[0]
best_mse = results_df['MSE'].iloc[0]

print(f"\nThe best performing model is {best_model_name} with an R2 score of {best_r2:.4f} and an MSE of {best_mse:.4f}.")
print("This model performed best because it has the highest R2 score and the lowest MSE among the evaluated models.")

"""## Predict and export scores

**Reasoning**:
Train the best model on the entire dataset, predict scores, create a new DataFrame with wallet and predicted score, and export to CSV.
"""

# Train the best performing model (RandomForestRegressor) on the entire dataset
best_model = RandomForestRegressor(random_state=42)
best_model.fit(X, y)

# Use the trained model to predict the credit scores for all wallets
wallet_features['predicted_credit_score'] = best_model.predict(X)

# Create a new DataFrame with wallet address and predicted score
wallet_scores = wallet_features[['predicted_credit_score']].copy()
wallet_scores = wallet_scores.reset_index().rename(columns={'index': 'userWallet'})

# Export to CSV
wallet_scores.to_csv('wallet_credit_scores.csv', index=False)

"""## Summary:

### Data Analysis Key Findings

*   Feature engineering successfully created features such as total transactions, total USD value, temporal features (active days, transactions per day), and counts/ratios of different transaction actions (deposit, borrow, liquidationcall, redeemunderlying, repay).
*   An initial credit score was calculated based on the log10 scaled total USD value, ranging from 0 to 1000.
*   The data was split into training and testing sets with a 80/20 ratio, and features were scaled using `StandardScaler`.
*   Four regression models (Random Forest, Gradient Boosting, XGBoost, and Support Vector Regressor) were trained and evaluated.
*   Random Forest Regressor demonstrated the best performance with the highest R2 score (close to 1) and lowest MSE among the evaluated models. Gradient Boosting and XGBoost also performed well, while SVR's performance was significantly lower.
*   The best-performing model (Random Forest Regressor) was retrained on the entire dataset and used to predict credit scores for all wallets.
*   A CSV file named `wallet_credit_scores.csv` was generated containing wallet addresses and their predicted credit scores.
*   A README section explaining the credit score logic (based on log-scaled total USD value and refined by the Random Forest model), the chosen model, key features, and the output file was created.



"""



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the wallet_credit_scores.csv file
wallet_scores = pd.read_csv('wallet_credit_scores.csv')

# Plot the distribution of predicted credit scores
plt.figure(figsize=(10, 6))
sns.histplot(wallet_scores['predicted_credit_score'], bins=50, kde=True)
plt.title('Distribution of Predicted Credit Scores')
plt.xlabel('Predicted Credit Score')
plt.ylabel('Frequency')
plt.show()

